{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import errno\n",
    "import torch\n",
    "import timeit\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from adamW import AdamW\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "from Loss import noisy_label_loss\n",
    "from Utilis import segmentation_scores, CustomDataset_punet, calculate_cm\n",
    "from Utilis import evaluate_noisy_label_4, evaluate_noisy_label_5, evaluate_noisy_label_6\n",
    "# our proposed model:\n",
    "from Models import UNet_CMs\n",
    "from PIL import Image"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# ========================= #\n",
    "# Hyper-parameters setting\n",
    "# ========================= #\n",
    "\n",
    "# hyper-parameters for model:\n",
    "input_dim = 3 # dimension of input\n",
    "width = 24 # width of the network\n",
    "depth = 3 # depth of the network, downsampling times is (depth-1)\n",
    "class_no = 2 # class number, 2 for binary\n",
    "\n",
    "# hyper-parameters for training:\n",
    "train_batchsize = 5 # batch size\n",
    "alpha = 0.001 # weight of the trace regularisation of learnt confusion matrices\n",
    "num_epochs = 40 # total epochs\n",
    "learning_rate = 1e-2 # learning rate"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# ======================================= #\n",
    "# Prepare a few data examples from MNIST \n",
    "# ======================================= #\n",
    "\n",
    "# Change path for your own datasets here:\n",
    "data_path = './MNIST_examples'\n",
    "dataset_tag = 'mnist'\n",
    "label_mode = 'multi'\n",
    "\n",
    "# full path to train/validate/test:\n",
    "test_path = data_path + '/test' \n",
    "train_path = data_path + '/train'\n",
    "validate_path = data_path + '/validate'\n",
    "\n",
    "# prepare data sets using our customdataset\n",
    "train_dataset = CustomDataset_punet(dataset_location=train_path, dataset_tag=dataset_tag, noisylabel=label_mode, augmentation=True)\n",
    "validate_dataset = CustomDataset_punet(dataset_location=validate_path, dataset_tag=dataset_tag, noisylabel=label_mode, augmentation=False)\n",
    "test_dataset = CustomDataset_punet(dataset_location=test_path, dataset_tag=dataset_tag, noisylabel=label_mode, augmentation=False)\n",
    "\n",
    "# putting dataset into data loaders\n",
    "trainloader = data.DataLoader(train_dataset, batch_size=train_batchsize, shuffle=True, num_workers=2, drop_last=True)\n",
    "validateloader = data.DataLoader(validate_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "testloader = data.DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "# demonstrate the training samples:\n",
    "Image_index_to_demonstrate = 6\n",
    "images, labels_over, labels_under, labels_wrong, labels_good, imagename = validate_dataset[Image_index_to_demonstrate]\n",
    "print('The dimension of image, channel:' + str(np.shape(images)[0]) + ', height:' + str(np.shape(images)[1]) + ', width:' + str(np.shape(images)[2]))\n",
    "print('The dimension of label, channel:' + str(np.shape(labels_over)[0]) + ', height:' + str(np.shape(labels_over)[1]) + ', width:' + str(np.shape(labels_over)[2]))\n",
    "\n",
    "# plot input image:\n",
    "# the input image is original mnist images with gaussian noises\n",
    "plt.imshow(np.mean(images, axis=0), cmap='gray')\n",
    "plt.title('Input image')\n",
    "plt.show()\n",
    "\n",
    "# plot the labels:\n",
    "fig = plt.figure(figsize=(9, 13))\n",
    "columns = 2\n",
    "rows = 2\n",
    "ax = []\n",
    "labels = []\n",
    "labels_names = []\n",
    "labels.append(labels_over)\n",
    "labels.append(labels_under)\n",
    "labels.append(labels_wrong)\n",
    "labels.append(labels_good)\n",
    "labels_names.append('Over segmentation')\n",
    "labels_names.append('Under segmentation')\n",
    "labels_names.append('Wrong segmentation')\n",
    "labels_names.append('Good segmentation')\n",
    "\n",
    "for i in range(columns*rows):\n",
    "    label_ = labels[i][0, :, :]\n",
    "    ax.append(fig.add_subplot(rows, columns, i+1))\n",
    "    ax[-1].set_title(labels_names[i]) \n",
    "    plt.imshow(label_, cmap='gray')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# ===== #\n",
    "# Model\n",
    "# ===== #\n",
    "\n",
    "# call model:\n",
    "model = UNet_CMs(in_ch=input_dim, width=width, depth=depth, class_no=class_no, norm='in', low_rank=False)\n",
    "\n",
    "# model name for saving:\n",
    "model_name = 'UNet_Confusion_Matrices_' + '_width' + str(width) + \\\n",
    "           '_depth' + str(depth) + '_train_batch_' + str(train_batchsize) + \\\n",
    "           '_alpha_' + str(alpha) + '_e' + str(num_epochs) + \\\n",
    "           '_lr' + str(learning_rate) \n",
    "\n",
    "# setting up device:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# =================================================== #\n",
    "# Prepare folders to save trained models and results \n",
    "# =================================================== #\n",
    "\n",
    "# save location:\n",
    "saved_information_path = './Results'\n",
    "try:\n",
    "    os.mkdir(saved_information_path)\n",
    "except OSError as exc:\n",
    "    if exc.errno != errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "\n",
    "saved_information_path = saved_information_path + '/' + model_name\n",
    "try:\n",
    "    os.mkdir(saved_information_path)\n",
    "except OSError as exc:\n",
    "    if exc.errno != errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "\n",
    "saved_model_path = saved_information_path + '/trained_models'\n",
    "try:\n",
    "    os.mkdir(saved_model_path)\n",
    "except OSError as exc:\n",
    "    if exc.errno != errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "\n",
    "save_path_visual_result = saved_information_path + '/visual_results'\n",
    "try:\n",
    "    os.mkdir(save_path_visual_result)\n",
    "except OSError as exc:\n",
    "    if exc.errno != errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "\n",
    "# tensorboardX file saved location:\n",
    "writer = SummaryWriter('./Results/Log_' + model_name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# =================================================== #\n",
    "# Training\n",
    "# =================================================== #\n",
    "\n",
    "# We use adamW optimiser for more accurate L2 regularisation\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_loss_ce = 0\n",
    "    running_loss_trace = 0\n",
    "    running_iou = 0\n",
    "    \n",
    "    for j, (images, labels_over, labels_under, labels_wrong, labels_good, imagename) in enumerate(trainloader):\n",
    "        \n",
    "        b, c, h, w = images.size()\n",
    "        \n",
    "        # zero graidents before each iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # cast numpy data into tensor float\n",
    "        images = images.to(device=device, dtype=torch.float32)\n",
    "        labels_over = labels_over.to(device=device, dtype=torch.float32)\n",
    "        labels_under = labels_under.to(device=device, dtype=torch.float32)\n",
    "        labels_wrong = labels_wrong.to(device=device, dtype=torch.float32)\n",
    "        labels_good = labels_good.to(device=device, dtype=torch.float32)\n",
    "        \n",
    "        labels_all = []\n",
    "        labels_all.append(labels_over)\n",
    "        labels_all.append(labels_under)\n",
    "        labels_all.append(labels_wrong)\n",
    "        labels_all.append(labels_good)\n",
    "        \n",
    "        # model has two outputs: \n",
    "        # first one is the probability map for true ground truth \n",
    "        # second one is a list collection of probability maps for different noisy ground truths\n",
    "        outputs_logits, outputs_logits_noisy = model(images)\n",
    "        \n",
    "        # calculate loss:\n",
    "        # loss: total loss\n",
    "        # loss_ce: main cross entropy loss\n",
    "        # loss_trace: regularisation loss\n",
    "        loss, loss_ce, loss_trace = noisy_label_loss(outputs_logits, outputs_logits_noisy, labels_all, alpha)\n",
    "\n",
    "        # calculate the gradients:\n",
    "        loss.backward()\n",
    "        # update weights in model:\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, train_output = torch.max(outputs_logits, dim=1)\n",
    "        train_iou = segmentation_scores(labels_good.cpu().detach().numpy(), train_output.cpu().detach().numpy(), class_no)\n",
    "        running_loss += loss\n",
    "        running_loss_ce += loss_ce\n",
    "        running_loss_trace += loss_trace\n",
    "        running_iou += train_iou\n",
    "\n",
    "        if (j + 1) == 1:\n",
    "            # check the validation accuray at the begning of each epoch:\n",
    "            v_dice, v_ged = evaluate_noisy_label_4(data=validateloader,\n",
    "                                                   model1=model,\n",
    "                                                   class_no=class_no)\n",
    "            \n",
    "            print(\n",
    "                'Step [{}/{}], '\n",
    "                'Val dice: {:.4f},'\n",
    "                'Val GED: {:.4f},'\n",
    "                'loss main: {:.4f},'\n",
    "                'loss regualrisation: {:.4f},'.format(epoch + 1, num_epochs,\n",
    "                                                            v_dice,\n",
    "                                                            v_ged,\n",
    "                                                            running_loss_ce / (j + 1),\n",
    "                                                            running_loss_trace / (j + 1)))\n",
    "        \n",
    "            writer.add_scalars('scalars', {'loss': running_loss / (j + 1),\n",
    "                                           'train iou': running_iou / (j + 1),\n",
    "                                           'val iou': v_dice,\n",
    "                                           'train main loss': running_loss_ce / (j + 1),\n",
    "                                           'train regularisation loss': running_loss_trace / (j + 1)}, epoch + 1)\n",
    "\n",
    "# save model:\n",
    "save_model_name_full = saved_model_path + '/' + model_name + '_Final.pt'\n",
    "torch.save(model, save_model_name_full)\n",
    "print('\\n')\n",
    "print('Training ended')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# =================================================== #\n",
    "# Testing\n",
    "# =================================================== #\n",
    "model.eval()\n",
    "for i, (v_images, labels_over, labels_under, labels_wrong, labels_good, imagename) in enumerate(testloader):\n",
    "        v_images = v_images.to(device=device, dtype=torch.float32)\n",
    "        v_outputs_logits_original, v_outputs_logits_noisy = model(v_images)\n",
    "        b, c, h, w = v_outputs_logits_original.size()\n",
    "        # plot the final segmentation map\n",
    "        v_outputs_logits_original = nn.Softmax(dim=1)(v_outputs_logits_original)\n",
    "        _, v_outputs_logits = torch.max(v_outputs_logits_original, dim=1)\n",
    "\n",
    "        save_name = save_path_visual_result + '/test_' + str(i) + '_seg.png'\n",
    "        save_name_label = save_path_visual_result + '/test_' + str(i) + '_label.png'\n",
    "        save_name_slice = save_path_visual_result + '/test_' + str(i) + '_img.png'\n",
    "\n",
    "        plt.imsave(save_name_slice, v_images[:, 1, :, :].reshape(h, w).cpu().detach().numpy(), cmap='gray')\n",
    "        plt.imsave(save_name, v_outputs_logits.reshape(h, w).cpu().detach().numpy(), cmap='gray')\n",
    "        plt.imsave(save_name_label, labels_good.reshape(h, w).cpu().detach().numpy(), cmap='gray')\n",
    "        \n",
    "        # plot the noisy segmentation maps:\n",
    "        v_outputs_logits_original = v_outputs_logits_original.reshape(b, c, h*w)\n",
    "        v_outputs_logits_original = v_outputs_logits_original.permute(0, 2, 1).contiguous()\n",
    "        v_outputs_logits_original = v_outputs_logits_original.view(b * h * w, c).view(b*h*w, c, 1)\n",
    "        for j, cm in enumerate(v_outputs_logits_noisy):\n",
    "            cm = cm.view(b, c**2, h*w).permute(0, 2, 1).contiguous().view(b*h*w, c*c).view(b*h*w, c, c)\n",
    "            cm = cm / cm.sum(1, keepdim=True)\n",
    "            v_noisy_output_original = torch.bmm(cm, v_outputs_logits_original).view(b*h*w, c)\n",
    "            v_noisy_output_original = v_noisy_output_original.view(b, h*w, c).permute(0, 2, 1).contiguous().view(b, c, h, w)\n",
    "            _, v_noisy_output = torch.max(v_noisy_output_original, dim=1)\n",
    "            save_name = save_path_visual_result + '/test_' + str(i) + '_noisy_' + str(j) + '_seg.png'\n",
    "            plt.imsave(save_name, v_noisy_output.reshape(h, w).cpu().detach().numpy(), cmap='gray')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# =================================================== #\n",
    "# Predictions Plot\n",
    "# =================================================== #\n",
    "test_data_index = 15\n",
    "\n",
    "over_seg = save_path_visual_result + '/test_' + str(test_data_index) + '_noisy_' + str(0) + '_seg.png'\n",
    "under_seg = save_path_visual_result + '/test_' + str(test_data_index) + '_noisy_' + str(1) + '_seg.png'\n",
    "wrong_seg = save_path_visual_result + '/test_' + str(test_data_index) + '_noisy_' + str(2) + '_seg.png'\n",
    "good_seg = save_path_visual_result + '/test_' + str(test_data_index) + '_noisy_' + str(3) + '_seg.png'\n",
    "\n",
    "seg = save_path_visual_result + '/test_' + str(test_data_index) + '_seg.png'\n",
    "label = save_path_visual_result + '/test_' + str(test_data_index) + '_label.png'\n",
    "img = save_path_visual_result + '/test_' + str(test_data_index) + '_img.png'\n",
    "\n",
    "# plot image, ground truth and final segmentation\n",
    "fig = plt.figure(figsize=(6.7, 13))\n",
    "columns = 3\n",
    "rows = 1\n",
    "\n",
    "ax = []\n",
    "imgs = []\n",
    "imgs_names = []\n",
    "\n",
    "imgs.append(img)\n",
    "imgs.append(label)\n",
    "imgs.append(seg)\n",
    "\n",
    "imgs_names.append('Test img')\n",
    "imgs_names.append('GroundTruth')\n",
    "imgs_names.append('Pred of true seg')\n",
    "\n",
    "for i in range(columns*rows):\n",
    "    img_ = imgs[i]\n",
    "    ax.append(fig.add_subplot(rows, columns, i+1))\n",
    "    ax[-1].set_title(imgs_names[i]) \n",
    "    img_ = Image.open(img_)\n",
    "    img_ = np.array(img_, dtype='uint8')\n",
    "    plt.imshow(img_, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# plot the segmentation for noisy labels:\n",
    "fig = plt.figure(figsize=(9, 13))\n",
    "columns = 4\n",
    "rows = 1\n",
    "\n",
    "ax = []\n",
    "noisy_segs = []\n",
    "noisy_segs_names = []\n",
    "\n",
    "noisy_segs.append(over_seg)\n",
    "noisy_segs.append(under_seg)\n",
    "noisy_segs.append(wrong_seg)\n",
    "noisy_segs.append(good_seg)\n",
    "\n",
    "noisy_segs_names.append('Pred of over')\n",
    "noisy_segs_names.append('Pred of under')\n",
    "noisy_segs_names.append('Pred of wrong')\n",
    "noisy_segs_names.append('Pred of good')\n",
    "\n",
    "for i in range(columns*rows):\n",
    "    noisy_seg_ = noisy_segs[i]\n",
    "    ax.append(fig.add_subplot(rows, columns, i+1))\n",
    "    ax[-1].set_title(noisy_segs_names[i]) \n",
    "    noisy_seg_ = Image.open(noisy_seg_)\n",
    "    noisy_seg_ = np.array(noisy_seg_, dtype='uint8' )\n",
    "    plt.imshow(noisy_seg_, cmap='gray')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
