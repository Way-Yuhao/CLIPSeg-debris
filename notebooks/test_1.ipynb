{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T21:54:29.976007Z",
     "start_time": "2024-09-05T21:54:20.305247Z"
    }
   },
   "source": [
    "import os\n",
    "import errno\n",
    "import torch\n",
    "import timeit\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from adamW import AdamW\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "from Loss import noisy_label_loss\n",
    "from Utilis import segmentation_scores, CustomDataset_punet, calculate_cm\n",
    "from Utilis import evaluate_noisy_label_4, evaluate_noisy_label_5, evaluate_noisy_label_6\n",
    "# our proposed model:\n",
    "from Models import UNet_CMs\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T21:54:29.991203Z",
     "start_time": "2024-09-05T21:54:29.977006Z"
    }
   },
   "source": [
    "# ========================= #\n",
    "# Hyper-parameters setting\n",
    "# ========================= #\n",
    "\n",
    "# hyper-parameters for model:\n",
    "input_dim = 3 # dimension of input\n",
    "width = 24 # width of the network\n",
    "depth = 3 # depth of the network, downsampling times is (depth-1)\n",
    "class_no = 2 # class number, 2 for binary\n",
    "\n",
    "# hyper-parameters for training:\n",
    "train_batchsize = 20   #5 # batch size\n",
    "alpha = 0.001 # weight of the trace regularisation of learnt confusion matrices\n",
    "num_epochs = 40 # total epochs\n",
    "learning_rate = 1e-2 # learning rate"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-09-05T21:54:30.338126Z",
     "start_time": "2024-09-05T21:54:29.992198Z"
    }
   },
   "source": [
    "# ======================================= #\n",
    "# Prepare a few data examples from MNIST \n",
    "# ======================================= #\n",
    "\n",
    "# Change path for your own datasets here:\n",
    "data_path = './MNIST_examples'\n",
    "dataset_tag = 'mnist'\n",
    "label_mode = 'multi'\n",
    "\n",
    "# full path to train/validate/test:\n",
    "test_path = data_path + '/test' \n",
    "train_path = data_path + '/train'\n",
    "validate_path = data_path + '/validate'\n",
    "\n",
    "# prepare data sets using our customdataset\n",
    "train_dataset = CustomDataset_punet(dataset_location=train_path, dataset_tag=dataset_tag, noisylabel=label_mode, augmentation=True)\n",
    "validate_dataset = CustomDataset_punet(dataset_location=validate_path, dataset_tag=dataset_tag, noisylabel=label_mode, augmentation=False)\n",
    "test_dataset = CustomDataset_punet(dataset_location=test_path, dataset_tag=dataset_tag, noisylabel=label_mode, augmentation=False)\n",
    "\n",
    "# putting dataset into data loaders\n",
    "trainloader = data.DataLoader(train_dataset, batch_size=train_batchsize, shuffle=True, num_workers=2, drop_last=True)\n",
    "validateloader = data.DataLoader(validate_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "testloader = data.DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "# demonstrate the training samples:\n",
    "Image_index_to_demonstrate = 6\n",
    "images, labels_over, labels_under, labels_wrong, labels_good, imagename = validate_dataset[Image_index_to_demonstrate]\n",
    "images = np.mean(images, axis=0)\n",
    "# print('The dimension of image, channel:' + str(np.shape(images)[0]) + ', height:' + str(np.shape(images)[1]) + ', width:' + str(np.shape(images)[2]))\n",
    "# print('The dimension of label, channel:' + str(np.shape(labels_over)[0]) + ', height:' + str(np.shape(labels_over)[1]) + ', width:' + str(np.shape(labels_over)[2]))\n",
    "\n",
    "# plot input image:\n",
    "# the input image is original mnist images with gaussian noises\n",
    "# plt.imshow(np.mean(images, axis=0), cmap='gray')\n",
    "# plt.title('Input image')\n",
    "# plt.show()\n",
    "\n",
    "# plot the labels:\n",
    "fig = plt.figure(figsize=(9, 13))\n",
    "columns = 5\n",
    "rows = 1\n",
    "ax = []\n",
    "labels = []\n",
    "labels_names = []\n",
    "labels.append(images)\n",
    "labels.append(labels_over)\n",
    "labels.append(labels_under)\n",
    "labels.append(labels_wrong)\n",
    "labels.append(labels_good)\n",
    "labels_names.append('Input')\n",
    "labels_names.append('Over label')\n",
    "labels_names.append('Under label')\n",
    "labels_names.append('Wrong label')\n",
    "labels_names.append('Good label')\n",
    "\n",
    "for i in range(columns*rows):\n",
    "    if i != 0:\n",
    "        label_ = labels[i][0, :, :]\n",
    "    else:\n",
    "        label_ = labels[i]\n",
    "    ax.append(fig.add_subplot(rows, columns, i+1))\n",
    "    ax[-1].set_title(labels_names[i]) \n",
    "    plt.imshow(label_, cmap='gray')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 21\u001B[0m\n\u001B[0;32m     18\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m CustomDataset_punet(dataset_location\u001B[38;5;241m=\u001B[39mtest_path, dataset_tag\u001B[38;5;241m=\u001B[39mdataset_tag, noisylabel\u001B[38;5;241m=\u001B[39mlabel_mode, augmentation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# putting dataset into data loaders\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m trainloader \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataLoader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_batchsize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrop_last\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m validateloader \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mDataLoader(validate_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     23\u001B[0m testloader \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mDataLoader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\merge_labels\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:351\u001B[0m, in \u001B[0;36mDataLoader.__init__\u001B[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001B[0m\n\u001B[0;32m    349\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# map-style\u001B[39;00m\n\u001B[0;32m    350\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m shuffle:\n\u001B[1;32m--> 351\u001B[0m         sampler \u001B[38;5;241m=\u001B[39m \u001B[43mRandomSampler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m    352\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    353\u001B[0m         sampler \u001B[38;5;241m=\u001B[39m SequentialSampler(dataset)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\merge_labels\\lib\\site-packages\\torch\\utils\\data\\sampler.py:144\u001B[0m, in \u001B[0;36mRandomSampler.__init__\u001B[1;34m(self, data_source, replacement, num_samples, generator)\u001B[0m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreplacement should be a boolean value, but got replacement=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplacement\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_samples\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===== #\n",
    "# Model\n",
    "# ===== #\n",
    "\n",
    "# call model:\n",
    "model = UNet_CMs(in_ch=input_dim, width=width, depth=depth, class_no=class_no, norm='in', low_rank=False)\n",
    "\n",
    "# model name for saving:\n",
    "model_name = 'UNet_Confusion_Matrices_' + '_width' + str(width) + \\\n",
    "           '_depth' + str(depth) + '_train_batch_' + str(train_batchsize) + \\\n",
    "           '_alpha_' + str(alpha) + '_e' + str(num_epochs) + \\\n",
    "           '_lr' + str(learning_rate) \n",
    "\n",
    "# setting up device:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =================================================== #\n",
    "# Prepare folders to save trained models and results \n",
    "# =================================================== #\n",
    "\n",
    "# save location:\n",
    "saved_information_path = './Results'\n",
    "try:\n",
    "    os.mkdir(saved_information_path)\n",
    "except OSError as exc:\n",
    "    if exc.errno != errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "\n",
    "saved_information_path = saved_information_path + '/' + model_name\n",
    "try:\n",
    "    os.mkdir(saved_information_path)\n",
    "except OSError as exc:\n",
    "    if exc.errno != errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "\n",
    "saved_model_path = saved_information_path + '/trained_models'\n",
    "try:\n",
    "    os.mkdir(saved_model_path)\n",
    "except OSError as exc:\n",
    "    if exc.errno != errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "\n",
    "save_path_visual_result = saved_information_path + '/visual_results'\n",
    "try:\n",
    "    os.mkdir(save_path_visual_result)\n",
    "except OSError as exc:\n",
    "    if exc.errno != errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "\n",
    "# tensorboardX file saved location:\n",
    "writer = SummaryWriter('./Results/Log_' + model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =================================================== #\n",
    "# Training\n",
    "# =================================================== #\n",
    "\n",
    "# We use adamW optimiser for more accurate L2 regularisation\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_loss_ce = 0\n",
    "    running_loss_trace = 0\n",
    "    running_iou = 0\n",
    "    \n",
    "    for j, (images, labels_over, labels_under, labels_wrong, labels_good, imagename) in enumerate(trainloader):\n",
    "        \n",
    "        b, c, h, w = images.size()\n",
    "        \n",
    "        # zero graidents before each iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # cast numpy data into tensor float\n",
    "        images = images.to(device=device, dtype=torch.float32)\n",
    "        labels_over = labels_over.to(device=device, dtype=torch.float32)\n",
    "        labels_under = labels_under.to(device=device, dtype=torch.float32)\n",
    "        labels_wrong = labels_wrong.to(device=device, dtype=torch.float32)\n",
    "        labels_good = labels_good.to(device=device, dtype=torch.float32)\n",
    "        \n",
    "        labels_all = []\n",
    "        labels_all.append(labels_over)\n",
    "        labels_all.append(labels_under)\n",
    "        labels_all.append(labels_wrong)\n",
    "        labels_all.append(labels_good)\n",
    "        \n",
    "        # model has two outputs: \n",
    "        # first one is the probability map for true ground truth \n",
    "        # second one is a list collection of probability maps for different noisy ground truths\n",
    "        outputs_logits, outputs_logits_noisy = model(images)\n",
    "        \n",
    "        # calculate loss:\n",
    "        # loss: total loss\n",
    "        # loss_ce: main cross entropy loss\n",
    "        # loss_trace: regularisation loss\n",
    "        loss, loss_ce, loss_trace = noisy_label_loss(outputs_logits, outputs_logits_noisy, labels_all, alpha)\n",
    "\n",
    "        # calculate the gradients:\n",
    "        loss.backward()\n",
    "        # update weights in model:\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, train_output = torch.max(outputs_logits, dim=1)\n",
    "        train_iou = segmentation_scores(labels_good.cpu().detach().numpy(), train_output.cpu().detach().numpy(), class_no)\n",
    "        running_loss += loss\n",
    "        running_loss_ce += loss_ce\n",
    "        running_loss_trace += loss_trace\n",
    "        running_iou += train_iou\n",
    "\n",
    "        if (j + 1) == 1:\n",
    "            # check the validation accuray at the begning of each epoch:\n",
    "            v_dice, v_ged = evaluate_noisy_label_4(data=validateloader,\n",
    "                                                   model1=model,\n",
    "                                                   class_no=class_no)\n",
    "            \n",
    "            print(\n",
    "                'Step [{}/{}], '\n",
    "                'Val dice: {:.4f},'\n",
    "                'Val GED: {:.4f},'\n",
    "                'loss main: {:.4f},'\n",
    "                'loss regularization: {:.4f},'.format(epoch + 1, num_epochs,\n",
    "                                                            v_dice,\n",
    "                                                            v_ged,\n",
    "                                                            running_loss_ce / (j + 1),\n",
    "                                                            running_loss_trace / (j + 1)))\n",
    "        \n",
    "            writer.add_scalars('scalars', {'loss': running_loss / (j + 1),\n",
    "                                           'train iou': running_iou / (j + 1),\n",
    "                                           'val iou': v_dice,\n",
    "                                           'train main loss': running_loss_ce / (j + 1),\n",
    "                                           'train regularization loss': running_loss_trace / (j + 1)}, epoch + 1)\n",
    "\n",
    "# save model:\n",
    "save_model_name_full = saved_model_path + '/' + model_name + '_Final.pt'\n",
    "torch.save(model, save_model_name_full)\n",
    "print('\\n')\n",
    "print('Training ended')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =================================================== #\n",
    "# Testing\n",
    "# =================================================== #\n",
    "model.eval()\n",
    "for i, (v_images, labels_over, labels_under, labels_wrong, labels_good, imagename) in enumerate(testloader):\n",
    "        v_images = v_images.to(device=device, dtype=torch.float32)\n",
    "        v_outputs_logits_original, v_outputs_logits_noisy = model(v_images)\n",
    "        b, c, h, w = v_outputs_logits_original.size()\n",
    "        # plot the final segmentation map\n",
    "        v_outputs_logits_original = nn.Softmax(dim=1)(v_outputs_logits_original)\n",
    "        _, v_outputs_logits = torch.max(v_outputs_logits_original, dim=1)\n",
    "\n",
    "        save_name = save_path_visual_result + '/test_' + str(i) + '_seg.png'\n",
    "        save_name_label = save_path_visual_result + '/test_' + str(i) + '_label.png'\n",
    "        save_name_slice = save_path_visual_result + '/test_' + str(i) + '_img.png'\n",
    "\n",
    "        plt.imsave(save_name_slice, v_images[:, 1, :, :].reshape(h, w).cpu().detach().numpy(), cmap='gray')\n",
    "        plt.imsave(save_name, v_outputs_logits.reshape(h, w).cpu().detach().numpy(), cmap='gray')\n",
    "        plt.imsave(save_name_label, labels_good.reshape(h, w).cpu().detach().numpy(), cmap='gray')\n",
    "        \n",
    "        # plot the noisy segmentation maps:\n",
    "        v_outputs_logits_original = v_outputs_logits_original.reshape(b, c, h*w)\n",
    "        v_outputs_logits_original = v_outputs_logits_original.permute(0, 2, 1).contiguous()\n",
    "        v_outputs_logits_original = v_outputs_logits_original.view(b * h * w, c).view(b*h*w, c, 1)\n",
    "        for j, cm in enumerate(v_outputs_logits_noisy):\n",
    "            cm = cm.view(b, c**2, h*w).permute(0, 2, 1).contiguous().view(b*h*w, c*c).view(b*h*w, c, c)\n",
    "            cm = cm / cm.sum(1, keepdim=True)\n",
    "            v_noisy_output_original = torch.bmm(cm, v_outputs_logits_original).view(b*h*w, c)\n",
    "            v_noisy_output_original = v_noisy_output_original.view(b, h*w, c).permute(0, 2, 1).contiguous().view(b, c, h, w)\n",
    "            _, v_noisy_output = torch.max(v_noisy_output_original, dim=1)\n",
    "            save_name = save_path_visual_result + '/test_' + str(i) + '_noisy_' + str(j) + '_seg.png'\n",
    "            plt.imsave(save_name, v_noisy_output.reshape(h, w).cpu().detach().numpy(), cmap='gray')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =================================================== #\n",
    "# Predictions Plot\n",
    "# =================================================== #\n",
    "test_data_index = 15\n",
    "\n",
    "over_seg = save_path_visual_result + '/test_' + str(test_data_index) + '_noisy_' + str(0) + '_seg.png'\n",
    "under_seg = save_path_visual_result + '/test_' + str(test_data_index) + '_noisy_' + str(1) + '_seg.png'\n",
    "wrong_seg = save_path_visual_result + '/test_' + str(test_data_index) + '_noisy_' + str(2) + '_seg.png'\n",
    "good_seg = save_path_visual_result + '/test_' + str(test_data_index) + '_noisy_' + str(3) + '_seg.png'\n",
    "\n",
    "seg = save_path_visual_result + '/test_' + str(test_data_index) + '_seg.png'\n",
    "label = save_path_visual_result + '/test_' + str(test_data_index) + '_label.png'\n",
    "img = save_path_visual_result + '/test_' + str(test_data_index) + '_img.png'\n",
    "\n",
    "# plot image, ground truth and final segmentation\n",
    "fig = plt.figure(figsize=(6.7, 13))\n",
    "columns = 3\n",
    "rows = 1\n",
    "\n",
    "ax = []\n",
    "imgs = []\n",
    "imgs_names = []\n",
    "\n",
    "imgs.append(img)\n",
    "imgs.append(label)\n",
    "imgs.append(seg)\n",
    "\n",
    "imgs_names.append('Test img')\n",
    "imgs_names.append('GroundTruth')\n",
    "imgs_names.append('Pred of true seg')\n",
    "\n",
    "for i in range(columns*rows):\n",
    "    img_ = imgs[i]\n",
    "    ax.append(fig.add_subplot(rows, columns, i+1))\n",
    "    ax[-1].set_title(imgs_names[i]) \n",
    "    img_ = Image.open(img_)\n",
    "    img_ = np.array(img_, dtype='uint8')\n",
    "    plt.imshow(img_, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# plot the segmentation for noisy labels:\n",
    "fig = plt.figure(figsize=(9, 13))\n",
    "columns = 4\n",
    "rows = 1\n",
    "\n",
    "ax = []\n",
    "noisy_segs = []\n",
    "noisy_segs_names = []\n",
    "\n",
    "noisy_segs.append(over_seg)\n",
    "noisy_segs.append(under_seg)\n",
    "noisy_segs.append(wrong_seg)\n",
    "noisy_segs.append(good_seg)\n",
    "\n",
    "noisy_segs_names.append('Pred of over')\n",
    "noisy_segs_names.append('Pred of under')\n",
    "noisy_segs_names.append('Pred of wrong')\n",
    "noisy_segs_names.append('Pred of good')\n",
    "\n",
    "for i in range(columns*rows):\n",
    "    noisy_seg_ = noisy_segs[i]\n",
    "    ax.append(fig.add_subplot(rows, columns, i+1))\n",
    "    ax[-1].set_title(noisy_segs_names[i]) \n",
    "    noisy_seg_ = Image.open(noisy_seg_)\n",
    "    noisy_seg_ = np.array(noisy_seg_, dtype='uint8' )\n",
    "    plt.imshow(noisy_seg_, cmap='gray')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
